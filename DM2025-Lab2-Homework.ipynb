{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Student Information**\n",
    "Name:劉明捷\n",
    "\n",
    "Student ID:114033632\n",
    "\n",
    "GitHub ID:JackLiu1208\n",
    "\n",
    "Kaggle name:jackliu\n",
    "\n",
    "Kaggle private scoreboard snapshot: \n",
    "\n",
    "![pic_ranking.png](./pics/Kaggle_ranking_result_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Instructions**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this lab we have divided the assignments into **three phases/parts**. The `first two phases` refer to the `exercises inside the Master notebooks` of the [DM2025-Lab2-Exercise Repo](https://github.com/difersalest/DM2025-Lab2-Exercise.git). The `third phase` refers to an `internal Kaggle competition` that we are gonna run among all the Data Mining students. Together they add up to `100 points` of your grade. There are also some `bonus points` to be gained if you complete `extra exercises` in the lab **(bonus 15 pts)** and in the `Kaggle Competition report` **(bonus 5 pts)**.\n",
    "\n",
    "**Environment recommendations to solve lab 2:**\n",
    "- **Phase 1 exercises:** Need GPU for training the models explained in that part, if you don't have a GPU in your laptop it is recommended to run in Colab or Kaggle for a faster experience, although with CPU they can still be solved but with a slower execution.\n",
    "- **Phase 2 exercises:** We use Gemini's API so everything can be run with only CPU without a problem.\n",
    "- **Phase 3 exercises:** For the competition you will probably need GPU to train your models, so it is recommended to use Colab or Kaggle if you don't have a laptop with a dedicated GPU.\n",
    "- **Optional Ollama Notebook (not graded):** You need GPU, at least 4GB of VRAM with 16 GB of RAM to run the local open-source LLM models. \n",
    "\n",
    "## **Phase 1 (30 pts):**\n",
    "\n",
    "1. __Main Exercises (25 pts):__ Do the **take home exercises** from Sections: `1. Data Preparation` to `9. High-dimension Visualization: t-SNE and UMAP`, in the [DM2025-Lab2-Master-Phase_1 Notebook](https://github.com/difersalest/DM2025-Lab2-Exercise/blob/main/DM2025-Lab2-Master-Phase_1.ipynb). Total: `8 exercises`. Commit your code and submit the repository link to NTU Cool **`BEFORE the deadline (Nov. 3th, 11:59 pm, Monday)`**\n",
    "\n",
    "2. **Code Comments (5 pts):** **Tidy up the code in your notebook**. \n",
    "\n",
    "## **Phase 2 (30 pts):**\n",
    "\n",
    "1. **Main Exercises (25 pts):** Do the remaining **take home exercises** from Section: `2. Large Language Models (LLMs)` in the [DM2025-Lab2-Master-Phase_2_Main Notebook](https://github.com/difersalest/DM2025-Lab2-Exercise/blob/main/DM2025-Lab2-Master-Phase_2_Main.ipynb). Total: `5 exercises required from sections 2.1, 2.2, 2.4 and 2.6`. Commit your code and submit the repository link to NTU Cool **`BEFORE the deadline (Nov. 24th, 11:59 pm, Monday)`**\n",
    "\n",
    "2. **Code Comments (5 pts):** **Tidy up the code in your notebook**. \n",
    "\n",
    "3. **`Bonus (15 pts):`** Complete the bonus exercises in the [DM2025-Lab2-Master-Phase_2_Bonus Notebook](https://github.com/difersalest/DM2025-Lab2-Exercise/blob/main/DM2025-Lab2-Master-Phase_2_Bonus.ipynb) and [DM2025-Lab2-Master-Phase_2_Main Notebook](https://github.com/difersalest/DM2025-Lab2-Exercise/blob/main/DM2025-Lab2-Master-Phase_2_Main.ipynb) `where 2 exercises are counted as bonus from sections 2.3 and 2.5 in the main notebook`. Total: `7 exercises`. Commit your code and submit the repository link to NTU Cool **`BEFORE the deadline (Nov. 24th, 11:59 pm, Monday)`**\n",
    "\n",
    "## **Phase 3 (40 pts):**\n",
    "\n",
    "1. **Kaggle Competition Participation (30 pts):** Participate in the in-class **Kaggle Competition** regarding Emotion Recognition on Twitter by clicking in this link: **[Data Mining Class Kaggle Competition](https://www.kaggle.com/t/3a2df4c6d6b4417e8bf718ed648d7554)**. The scoring will be given according to your place in the Private Leaderboard ranking: \n",
    "    - **Bottom 40%**: Get 20 pts of the 30 pts in this competition participation part.\n",
    "\n",
    "    - **Top 41% - 100%**: Get (0.6N + 1 - x) / (0.6N) * 10 + 20 points, where N is the total number of participants, and x is your rank. (ie. If there are 100 participants and you rank 3rd your score will be (0.6 * 100 + 1 - 3) / (0.6 * 100) * 10 + 20 = 29.67% out of 30%.)   \n",
    "    Submit your last submission **`BEFORE the deadline (Nov. 24th, 11:59 pm, Monday)`**. Make sure to take a screenshot of your position at the end of the competition and store it as `pic_ranking.png` under the `pics` folder of this repository and rerun the cell **Student Information**.\n",
    "\n",
    "2. **Competition Report (10 pts)** A report section to be filled in inside this notebook in Markdown Format, we already provided you with the template below. You need to describe your work developing the model for the competition. The report should include a section describing briefly the following elements: \n",
    "* Your preprocessing steps.\n",
    "* The feature engineering steps.\n",
    "* Explanation of your model.\n",
    "\n",
    "* **`Bonus (5 pts):`**\n",
    "    * You will have to describe more detail in the previous steps.\n",
    "    * Mention different things you tried.\n",
    "    * Mention insights you gained. \n",
    "\n",
    "[Markdown Guide - Basic Syntax](https://www.markdownguide.org/basic-syntax/)\n",
    "\n",
    "**`Things to note for Phase 3:`**\n",
    "\n",
    "* **The code used for the competition should be in this Jupyter Notebook File** `DM2025-Lab2-Homework.ipynb`.\n",
    "\n",
    "* **Push the code used for the competition to your repository**.\n",
    "\n",
    "* **The code should have a clear separation for the same sections of the report, preprocessing, feature engineering and model explanation. Briefly comment your code for easier understanding, we provide a template at the end of this notebook.**\n",
    "\n",
    "* Showing the kaggle screenshot of the ranking plus the code in this notebook will ensure the validity of your participation and the report to obtain the corresponding points.\n",
    "\n",
    "After the competition ends you will have two days more to submit the `DM2025-Lab2-Homework.ipynb` with your report in markdown format and your code. Do everything **`BEFORE the deadline (Nov. 26th, 11:59 pm, Wednesday) to obtain 100% of the available points.`**\n",
    "\n",
    "Upload your files to your repository then submit the link to it on the corresponding NTU Cool assignment.\n",
    "\n",
    "## **Deadlines:**\n",
    "\n",
    "![lab2_deadlines](./pics/lab2_deadlines.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Next you will find the template report with some simple markdown syntax explanations, use it to structure your content.\n",
    "\n",
    "You can delete the syntax suggestions after you use them.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "# **Project Report**\n",
    "\n",
    "**Syntax:** `#` creates the largest heading (H1).\n",
    "\n",
    "---\n",
    "**Syntax:** `---` creates a horizontal rule (a separator line).\n",
    "\n",
    "## 1. Model Development (10 pts Required)\n",
    "\n",
    "**Syntax:** `##` creates a secondary heading (H2).\n",
    "\n",
    "**Describe briefly each section, you can add graphs/charts to support your explanations.**\n",
    "\n",
    "### 1.1 Preprocessing Steps\n",
    "\n",
    "**Syntax:** `###` creates a tertiary heading (H3).\n",
    "\n",
    "[Content for Preprocessing]\n",
    "\n",
    "**Example Syntax for Content:**\n",
    "*   **Bold text:** `**text**`\n",
    "*   *Italic text*: `*text*`\n",
    "*   Bullet point list:\n",
    "    * Item 1\n",
    "    * Item 2\n",
    "\n",
    "Markdown Syntax to Add Image: `![Description of the Image](./your_local_folder/name_of_the_image.png)`\n",
    "\n",
    "![Example Markdown Syntax to Add Image](./pics/example_md_img.png)\n",
    "\n",
    "### 1.2 Feature Engineering Steps\n",
    "\n",
    "[Content for Feature Engineering]\n",
    "\n",
    "### 1.3 Explanation of Your Model\n",
    "\n",
    "[Content for Model Explanation]\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Bonus Section (5 pts Optional)\n",
    "\n",
    "**Add more detail in previous sections**\n",
    "\n",
    "### 2.1 Mention Different Things You Tried\n",
    "\n",
    "[Content for Experiments]\n",
    "\n",
    "### 2.2 Mention Insights You Gained\n",
    "\n",
    "[Content for Insights]\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`From here on starts the code section for the competition.`**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Competition Code**\n",
    "\n",
    "## 1. Preprocessing Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "情緒類別數量: 6\n",
      "情緒類別: ['anger' 'disgust' 'fear' 'joy' 'sadness' 'surprise']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# --- 1. 資料載入與合併 (與前一版本相同) ---\n",
    "\n",
    "# 檔案路徑（假設您將這些檔案放在專案根目錄或運行環境可訪問的位置）\n",
    "POSTS_FILE = \"Kaggle/final_posts.json\"\n",
    "ID_FILE = \"Kaggle/data_identification.csv\"\n",
    "EMOTION_FILE = \"Kaggle/emotion.csv\"\n",
    "SUBMISSION_FILE = 'submission_prediction.csv'\n",
    "MODEL_NAME = 'bert-base-uncased' # 選擇一個基礎的英文BERT模型\n",
    "\n",
    "# 載入 final_posts.json\n",
    "def load_posts_from_json(file_path):\n",
    "    posts = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        # 由於檔案結構是 list of dicts，直接 load\n",
    "        json_data = json.load(f)\n",
    "        for entry in json_data:\n",
    "            post_id = entry['root']['_source']['post']['post_id']\n",
    "            text = entry['root']['_source']['post']['text']\n",
    "            posts.append({'id': post_id, 'text': text})\n",
    "    return pd.DataFrame(posts)\n",
    "\n",
    "data_id_df = pd.read_csv(ID_FILE)\n",
    "emotion_df = pd.read_csv(EMOTION_FILE)\n",
    "all_posts_df = load_posts_from_json(POSTS_FILE)\n",
    "\n",
    "# 合併數據集\n",
    "merged_df = pd.merge(data_id_df, all_posts_df, on='id', how='left')\n",
    "full_df = pd.merge(merged_df, emotion_df, on='id', how='left')\n",
    "\n",
    "# 分割訓練集和測試集\n",
    "train_df = full_df[full_df['split'] == 'train'].copy().dropna(subset=['emotion']).reset_index(drop=True)\n",
    "test_df = full_df[full_df['split'] == 'test'].copy().reset_index(drop=True)\n",
    "\n",
    "# 簡單的文本清洗 (與 BERT Tokenizer 配合)\n",
    "def clean_text_for_bert(text):\n",
    "    text = str(text)\n",
    "    # 移除 [NAME], [RELIGION] 等標籤，但保留標點符號讓 BERT 處理\n",
    "    text = re.sub(r'\\[NAME\\]|\\[RELIGION\\]|\\[URL\\]', ' ', text)\n",
    "    # 移除 URL\n",
    "    text = re.sub(r'http\\S+', ' ', text)\n",
    "    # 移除多餘空格\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "train_df['cleaned_text'] = train_df['text'].apply(clean_text_for_bert)\n",
    "test_df['cleaned_text'] = test_df['text'].apply(clean_text_for_bert)\n",
    "\n",
    "# 標籤編碼 (Label Encoding)\n",
    "le = LabelEncoder()\n",
    "train_df['label'] = le.fit_transform(train_df['emotion'])\n",
    "\n",
    "# 獲取類別數量和類別名稱\n",
    "num_labels = len(le.classes_)\n",
    "id_to_label = {i: label for i, label in enumerate(le.classes_)}\n",
    "print(f\"情緒類別數量: {num_labels}\")\n",
    "print(f\"情緒類別: {le.classes_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature Engineering Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2. 自定義 PyTorch Dataset ---\n",
    "class EmotionDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        text = self.texts[item]\n",
    "        label = self.labels[item]\n",
    "\n",
    "        # BERT Tokenizer 負責將文本轉換為 input_ids, token_type_ids, attention_mask\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True, # 添加 [CLS] 和 [SEP]\n",
    "            max_length=self.max_len,\n",
    "            return_token_type_ids=True,\n",
    "            padding='max_length',\n",
    "            truncation=True, # 截斷超過最大長度的文本\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt', # 返回 PyTorch tensors\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'text': text,\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'token_type_ids': encoding['token_type_ids'].flatten(),\n",
    "            'label': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# 初始化 Tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(MODEL_NAME)\n",
    "MAX_LEN = 128 # 序列最大長度 (可根據數據分佈調整)\n",
    "BATCH_SIZE = 16 # 批次大小 (受限於 GPU 記憶體)\n",
    "\n",
    "# 創建訓練集和測試集 Dataset\n",
    "# 訓練集 labels 使用編碼後的數值標籤\n",
    "train_dataset = EmotionDataset(\n",
    "    texts=train_df['cleaned_text'].to_list(),\n",
    "    labels=train_df['label'].to_list(),\n",
    "    tokenizer=tokenizer,\n",
    "    max_len=MAX_LEN\n",
    ")\n",
    "\n",
    "# 測試集 labels 使用 -1 或 0 替代，因為我們不需要標籤，只為維持 Dataset 結構\n",
    "test_dataset = EmotionDataset(\n",
    "    texts=test_df['cleaned_text'].to_list(),\n",
    "    labels=[0] * len(test_df), # 測試集使用虛擬標籤\n",
    "    tokenizer=tokenizer,\n",
    "    max_len=MAX_LEN\n",
    ")\n",
    "\n",
    "# 創建 DataLoader\n",
    "train_data_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "test_data_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Implementation Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "使用裝置: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "開始 BERT 模型微調...\n",
      "\n",
      "--- Epoch 1/3 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f106120ca3af406788ad6227ea0397e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/2994 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "訓練損失 (Training Loss): 0.9258\n",
      "\n",
      "--- Epoch 2/3 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7cee1632a14477dbad355ac5d2f5ca1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/2994 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "訓練損失 (Training Loss): 0.7327\n",
      "\n",
      "--- Epoch 3/3 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c0cc0354fe945358ca506224d944fb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/2994 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "訓練損失 (Training Loss): 0.5309\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ccb7064ae7c4dacb5e63ebbd9d43bd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting:   0%|          | 0/1018 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "提交檔案前 5 行預覽:\n",
      "         id emotion\n",
      "0  0x61fc95    fear\n",
      "1  0xaba820     joy\n",
      "2  0x66e44d     joy\n",
      "3  0xc03cf5     joy\n",
      "4  0x02f65a   anger\n",
      "\n",
      "✅ 預測結果已儲存至: submission_prediction.csv\n"
     ]
    }
   ],
   "source": [
    "# --- 3. 模型訓練 (BERT Fine-Tuning) ---\n",
    "\n",
    "# 檢查是否有 GPU 可用\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"使用裝置: {device}\")\n",
    "\n",
    "# 載入 BERT 模型 (用於序列分類)\n",
    "model = BertForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=num_labels)\n",
    "model = model.to(device)\n",
    "\n",
    "# 設定優化器\n",
    "EPOCHS = 3 # 訓練輪次，BERT 通常只需要 2-4 個 Epochs\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5) # 學習率通常設為 1e-5 或 2e-5\n",
    "\n",
    "# 訓練函數\n",
    "def train_epoch(model, data_loader, optimizer, device, n_examples):\n",
    "    model = model.train()\n",
    "    losses = []\n",
    "    \n",
    "    for d in tqdm(data_loader, desc=\"Training\"):\n",
    "        # 將數據移動到指定裝置\n",
    "        input_ids = d[\"input_ids\"].to(device)\n",
    "        attention_mask = d[\"attention_mask\"].to(device)\n",
    "        token_type_ids = d[\"token_type_ids\"].to(device)\n",
    "        labels = d[\"label\"].to(device)\n",
    "\n",
    "        # 清除梯度\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # 進行前向傳播\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            token_type_ids=token_type_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels\n",
    "        )\n",
    "        \n",
    "        loss = outputs.loss\n",
    "        losses.append(loss.item())\n",
    "        \n",
    "        # 反向傳播和優化\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    return sum(losses) / len(losses)\n",
    "\n",
    "# 執行訓練\n",
    "print(\"開始 BERT 模型微調...\")\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f'\\n--- Epoch {epoch + 1}/{EPOCHS} ---')\n",
    "    train_loss = train_epoch(\n",
    "        model,\n",
    "        train_data_loader,\n",
    "        optimizer,\n",
    "        device,\n",
    "        len(train_df)\n",
    "    )\n",
    "    print(f'訓練損失 (Training Loss): {train_loss:.4f}')\n",
    "    \n",
    "# --- 4. 預測函數 ---\n",
    "def get_predictions(model, data_loader, device):\n",
    "    model = model.eval() # 設定為評估模式\n",
    "    predictions = []\n",
    "    \n",
    "    with torch.no_grad(): # 關閉梯度計算\n",
    "        for d in tqdm(data_loader, desc=\"Predicting\"):\n",
    "            input_ids = d[\"input_ids\"].to(device)\n",
    "            attention_mask = d[\"attention_mask\"].to(device)\n",
    "            token_type_ids = d[\"token_type_ids\"].to(device)\n",
    "\n",
    "            # 進行前向傳播\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                token_type_ids=token_type_ids,\n",
    "                attention_mask=attention_mask\n",
    "            )\n",
    "            \n",
    "            # 取得預測結果 ( logits)\n",
    "            _, preds = torch.max(outputs.logits, dim=1)\n",
    "            predictions.extend(preds.cpu().numpy().tolist())\n",
    "            \n",
    "    return predictions\n",
    "\n",
    "# 執行預測\n",
    "y_pred_encoded = get_predictions(model, test_data_loader, device)\n",
    "\n",
    "# 將數值預測轉換回原始的情緒標籤\n",
    "y_pred = le.inverse_transform(y_pred_encoded)\n",
    "test_df['emotion'] = y_pred\n",
    "\n",
    "# 5. 準備提交檔案\n",
    "submission_df = test_df[['id', 'emotion']]\n",
    "\n",
    "# 輸出前幾行作為檢查\n",
    "print(\"\\n提交檔案前 5 行預覽:\")\n",
    "print(submission_df.head())\n",
    "\n",
    "# 6. 儲存結果為 CSV\n",
    "submission_df.to_csv(SUBMISSION_FILE, index=False)\n",
    "\n",
    "print(f\"\\n✅ 預測結果已儲存至: {SUBMISSION_FILE}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DM2025-Lab2-Exercise",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
